{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from ipfn import ipfn\n",
    "from matplotlib import pyplot as plt\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "from sttn.data.lehd import OriginDestinationEmploymentDataProvider\n",
    "provider = OriginDestinationEmploymentDataProvider()\n",
    "\n",
    "import math\n",
    "from sttn.network import SpatioTemporalNetwork\n",
    "from sttn.utils import add_distance\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = [\n",
    "    ('New York City', 'ny', ['New York County, NY', 'Queens County, NY','Kings County, NY','Bronx County, NY','Richmond County, NY']),\n",
    "    ('Los Angeles', 'ca', ['Los Angeles County, CA']),\n",
    "    ('Chicago', 'il', ['Cook County, IL']),\n",
    "    ('Houston', 'tx', ['Harris County, TX']),\n",
    "    ('Boston', 'ma', ['Suffolk County, MA', 'Middlesex County, MA']),\n",
    "    ('Phoenix', 'az', ['Maricopa County, AZ']),\n",
    "    ('Philadelphia', 'pa', ['Philadelphia County, PA']),\n",
    "    ('San Antonio', 'tx', ['Bexar County, TX']),\n",
    "    ('San Diego', 'ca', ['San Diego County, CA']),\n",
    "    ('Dallas', 'tx', ['Dallas County, TX']),\n",
    "    ('San Jose', 'ca', ['Santa Clara County, CA']),\n",
    "    ('Austin', 'tx', ['Travis County, TX']),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data for above cities - census tract level\n",
    "\n",
    "allCity_dfs = []\n",
    "job_column = 'S000'\n",
    "comp_aggs={job_column: 'sum'}\n",
    "for city, state, conties in cities:\n",
    "    state_network = provider.get_data(state=state, year=2018)\n",
    "    city_network = state_network.filter_nodes(state_network.nodes.county.isin(conties))\n",
    "    with_distance = add_distance(city_network).edges\n",
    "    \n",
    "    city_jobs = city_network.agg_adjacent_edges(aggs=comp_aggs, outgoing=False).rename(columns={job_column: 'jobs'}).reset_index()\n",
    "    city_pop = city_network.agg_adjacent_edges(aggs=comp_aggs, outgoing=True).rename(columns={job_column: 'residence'}).reset_index()\n",
    "    \n",
    "    city_dist = with_distance.merge(city_jobs, on='destination')\n",
    "    city_cum = city_dist.merge(city_pop, on='origin')\n",
    "    \n",
    "    allCity_dfs.append(city_cum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘cities’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir cities\n",
    "for i,df in enumerate(allCity_dfs):\n",
    "    city = cities[i][0]\n",
    "    df.to_csv('cities/'+city+'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### log-mse doubly constrained model from Devashish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# function for bins\n",
    "def getbins(df, nbins=20):\n",
    "    \n",
    "    df.loc[df.distance == 0, 'distance'] = 0.2\n",
    "    \n",
    "    df['bin'] = pd.qcut(df['distance'], q=20)\n",
    "    df.sort_values(by='bin', inplace=True)\n",
    "    df.loc[df['SE01'] == 0, 'SE01'] = 0.1\n",
    "    df.loc[df['SE02'] == 0, 'SE02'] = 0.1\n",
    "    df.loc[df['SE03'] == 0, 'SE03'] = 0.1\n",
    "    \n",
    "    df.rename(columns={'jobs':'S000jobs', 'residence':'S000residence'}, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# doubly constrained model\n",
    "def constrained_model(data, separate_income=False):\n",
    "    \n",
    "    y_target = ['S000']  # target = total commute if no income segregation\n",
    "\n",
    "    if separate_income == True:\n",
    "        \n",
    "        y_target = ['SE01', 'SE02', 'SE03'] # target = individual income commute if income segregation\n",
    "        \n",
    "        origin = df.groupby(['origin']).agg({'SE01':sum,'SE02':sum,'SE03':sum}).reset_index()\n",
    "        origin.columns = ['origin','SE01residence','SE02residence','SE03residence']\n",
    "        destination = df.groupby(['destination']).agg({'SE01':sum,'SE02':sum,'SE03':sum}).reset_index()\n",
    "        destination.columns = ['destination','SE01jobs','SE02jobs','SE03jobs']\n",
    "        data = data.merge(origin,on=['origin'])\n",
    "        data = data.merge(destination,on=['destination'])\n",
    "    \n",
    "    dataF = []\n",
    "    dataUV = []\n",
    "    \n",
    "    for target in y_target:\n",
    "#         print(target)\n",
    "        allF = {}\n",
    "\n",
    "        # estimate F for each bin\n",
    "        for b in data['bin'].unique():\n",
    "\n",
    "            subData = data[data['bin'] == b]\n",
    "\n",
    "            X = sm.add_constant(np.log(subData[target+'residence']) + np.log(subData[target+'jobs']))\n",
    "            \n",
    "#             X = sm.add_constant(np.log(subData['residence']) + np.log(subData['jobs']))\n",
    "            y = subData[target]\n",
    "\n",
    "            model = sm.OLS(y,X).fit()\n",
    "\n",
    "            allF[b] = model.params[0]\n",
    "\n",
    "        binF = pd.DataFrame.from_dict(allF, orient='index', columns={'F'}).reset_index()\n",
    "        binF.rename(columns={'index':'bin'}, inplace=True)\n",
    "        \n",
    "        dataF.append(binF)\n",
    "        \n",
    "        allU = {}\n",
    "        allU_arr = []\n",
    "\n",
    "        # temporary dataframe with F\n",
    "        dataStep1 = data.merge(binF, on='bin', how='left') \n",
    "\n",
    "        # estimate V(o) for each origin\n",
    "        for o in data['origin'].unique():\n",
    "\n",
    "            subData = dataStep1[dataStep1['origin'] == o]\n",
    "            \n",
    "            X = np.log(subData[target+'residence']*subData['F'])\n",
    "            y = np.log(subData[target]) - np.log(subData[target+'jobs'])\n",
    "            \n",
    "\n",
    "            X = sm.add_constant(X)\n",
    "            model = sm.OLS(y,X).fit()\n",
    "\n",
    "            allU[o] = model.params[0]\n",
    "            allU_arr.append([model.params[0]]*len(subData))\n",
    "\n",
    "        binU = pd.DataFrame.from_dict(allU, orient='index', columns={'V'}).reset_index()\n",
    "        binU.rename(columns={'index':'origin'}, inplace=True)\n",
    "\n",
    "        # temporary dataframe with U\n",
    "        dataStep2 = dataStep1.merge(binU, on='origin', how='left') \n",
    "        # temp = dataStep2[['from_residents', 'V']].drop_duplicates()\n",
    "        dataStep2.loc[dataStep2.V < 0, 'V'] = 0.0001\n",
    "\n",
    "        # proportional fitting for V(o) for constraint sum(V) = sum(population)\n",
    "        Ptotal = [np.array(dataStep2.drop_duplicates(subset='origin')[target+'residence'].rename('total'))]\n",
    "        dimensions1 = [[0]]\n",
    "        pad = len(max(allU_arr, key=len))\n",
    "        vs = np.array([i + [0]*(pad-len(i)) for i in allU_arr])\n",
    "        IPF_1 = ipfn.ipfn(vs, Ptotal, dimensions1)\n",
    "        m = IPF_1.iteration()\n",
    "        temp1 = dataStep2.drop_duplicates(subset='origin')\n",
    "        temp1['m'] = m[:,0]\n",
    "        dataStep2 = dataStep2.merge(temp1[['origin', 'm']], on='origin', how='left')\n",
    "        dataStep2.loc[dataStep2.m == 0, 'm'] = 0.0001\n",
    "\n",
    "        allV = {}\n",
    "        allV_arr = []\n",
    "\n",
    "        # estimate U(d) for each destination\n",
    "        for d in data['destination'].unique():\n",
    "\n",
    "            subData = dataStep2[dataStep2['destination'] == d]\n",
    "\n",
    "            X = np.log(subData['m']*subData[target+'jobs']*subData['F'])\n",
    "            \n",
    "            y = np.log(subData[target])\n",
    "\n",
    "            X = sm.add_constant(X)\n",
    "            model = sm.OLS(y,X).fit()\n",
    "\n",
    "            allV[d] = model.params[0]\n",
    "            allV_arr.append([model.params[0]]*len(subData))\n",
    "\n",
    "        binV = pd.DataFrame.from_dict(allV, orient='index', columns={'U'}).reset_index()\n",
    "        binV.rename(columns={'index':'destination'}, inplace=True)\n",
    "\n",
    "        # temporary dataframe with U\n",
    "        dataStep3 = dataStep2.merge(binV, on='destination', how='left') \n",
    "\n",
    "        # proportional fitting for U(d) for constraint sum(U) = sum(jobs)\n",
    "        Wtotal = [np.array(dataStep3.drop_duplicates(subset='destination')[target+'jobs'].rename('total'))]\n",
    "        dimensions1 = [[0]]\n",
    "        pad = len(max(allV_arr, key=len))\n",
    "        us = np.array([i + [0]*(pad-len(i)) for i in allV_arr])\n",
    "        IPF_1 = ipfn.ipfn(us, Wtotal, dimensions1)\n",
    "        n = IPF_1.iteration()\n",
    "        \n",
    "        ## final dataframe with F, U, V\n",
    "        temp2 = dataStep3.drop_duplicates(subset='destination')\n",
    "        temp2['n'] = n[:,0]\n",
    "        dataStep3 = dataStep3.merge(temp2[['destination', 'n']], on='destination', how='left')\n",
    "        dataStep3.loc[dataStep3.n == 0, 'n'] = 0.0001\n",
    "        \n",
    "        dataUV.append(dataStep3)\n",
    "    \n",
    "    if separate_income == False:\n",
    "        return dataF[0], dataUV[0]\n",
    "    else:\n",
    "        return dataF, dataUV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘constrainCTdistbinsIncome/’: File exists\n",
      "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
      "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
      "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
      "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
      "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
      "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
      "ipfn converged: convergence_rate not updating or below rate_tolerance\n",
      "ipfn converged: convergence_rate not updating or below rate_tolerance\n"
     ]
    }
   ],
   "source": [
    "!mkdir constrainCTdistbinsIncome/\n",
    "import os\n",
    "cities = os.listdir('cities/')\n",
    "for city in cities:\n",
    "    if '.csv' in city:\n",
    "        df = pd.read_csv('cities/'+city)\n",
    "        dataF, dataUV = constrained_model(getbins(df),separate_income=True)\n",
    "        dataUV = pd.concat([i for i in dataUV])\n",
    "        dataUV['flowPred'] = dataUV['F']*dataUV['m']*dataUV['n']\n",
    "        dataUV = dataUV.groupby(['origin','destination']).agg({'S000':sum,'flowPred':sum}).reset_index()\n",
    "        dataUV = dataUV.rename(columns={'SOOO','flowReal'})\n",
    "        dataUV.to_csv('constrainCTdistbinsIncome/'+city,index=False)\n",
    "    #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir constrainCTdistbins/\n",
    "import os\n",
    "cities = os.listdir('cities/')\n",
    "for city in cities:\n",
    "    if '.csv' in city:\n",
    "        df = pd.read_csv('cities/'+city)\n",
    "        dataF, dataUV = constrained_model(getbins(df),separate_income=True)\n",
    "        dataUV = pd.concat([i for i in dataUV])\n",
    "        dataUV['flowPred'] = dataUV['F']*dataUV['m']*dataUV['n']\n",
    "        dataUV = dataUV.groupby(['origin','destination']).agg({'S000':sum,'flowPred':sum}).reset_index()\n",
    "        dataUV = dataUV.rename(columns={'SOOO','flowReal'})\n",
    "        dataUV.to_csv('constrainCTdistbins/'+city,index=False)\n",
    "    #     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mse doubly constrained model from Mingyi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def constrained_model_nonlog(data, separate_income=False):\n",
    "    \n",
    "    y_target = ['S000']  # target = total commute if no income segregation\n",
    "\n",
    "    \n",
    "    if separate_income == True:\n",
    "        \n",
    "        y_target = ['SE01', 'SE02', 'SE03'] # target = individual income commute if income segregation\n",
    "        \n",
    "        origin = data.groupby(['origin']).agg({'SE01':sum,'SE02':sum,'SE03':sum}).reset_index()\n",
    "        origin.columns = ['origin','SE01residence','SE02residence','SE03residence']\n",
    "        destination = data.groupby(['destination']).agg({'SE01':sum,'SE02':sum,'SE03':sum}).reset_index()\n",
    "        destination.columns = ['destination','SE01jobs','SE02jobs','SE03jobs']\n",
    "        data = data.merge(origin,on=['origin'])\n",
    "        data = data.merge(destination,on=['destination'])\n",
    "    \n",
    "    dataF = []\n",
    "    dataUV = []\n",
    "    \n",
    "    for target in y_target:\n",
    "#         print(target)\n",
    "        allF = {}\n",
    "\n",
    "        # estimate F for each bin\n",
    "        for b in data['bin'].unique():\n",
    "\n",
    "            subData = data[data['bin'] == b]\n",
    "\n",
    "            X = subData[target+'residence'] * subData[target+'jobs']\n",
    "            \n",
    "            y = subData[target]\n",
    "\n",
    "            model = sm.OLS(y,X).fit()\n",
    "            \n",
    "            allF[b] = model.params[0]\n",
    "\n",
    "        binF = pd.DataFrame.from_dict(allF, orient='index', columns={'F'}).reset_index()\n",
    "        binF.rename(columns={'index':'bin'}, inplace=True)\n",
    "        \n",
    "        dataF.append(binF)\n",
    "        \n",
    "        allU = {}\n",
    "        allU_arr = []\n",
    "\n",
    "        # temporary dataframe with F\n",
    "        dataStep1 = data.merge(binF, on='bin', how='left') \n",
    "\n",
    "        # estimate V(o) for each origin\n",
    "        for o in data['origin'].unique():\n",
    "\n",
    "            subData = dataStep1[dataStep1['origin'] == o]\n",
    "            \n",
    "            X = subData[target+'residence']*subData['F']*subData[target+'jobs']\n",
    "            y = subData[target] \n",
    "            \n",
    "\n",
    "            model = sm.OLS(y,X).fit()\n",
    "\n",
    "            allU[o] = model.params[0]\n",
    "            allU_arr.append([model.params[0]]*len(subData))\n",
    "\n",
    "        binU = pd.DataFrame.from_dict(allU, orient='index', columns={'V'}).reset_index()\n",
    "        binU.rename(columns={'index':'origin'}, inplace=True)\n",
    "\n",
    "        # temporary dataframe with U\n",
    "        dataStep2 = dataStep1.merge(binU, on='origin', how='left') \n",
    "        # temp = dataStep2[['from_residents', 'V']].drop_duplicates()\n",
    "        dataStep2.loc[dataStep2.V < 0, 'V'] = 0.0001\n",
    "\n",
    "        # proportional fitting for V(o) for constraint sum(V) = sum(population)\n",
    "        Ptotal = [np.array(dataStep2.drop_duplicates(subset='origin')[target+'residence'].rename('total'))]\n",
    "        dimensions1 = [[0]]\n",
    "        pad = len(max(allU_arr, key=len))\n",
    "        vs = np.array([i + [0]*(pad-len(i)) for i in allU_arr])\n",
    "        IPF_1 = ipfn.ipfn(vs, Ptotal, dimensions1)\n",
    "        m = IPF_1.iteration()\n",
    "        temp1 = dataStep2.drop_duplicates(subset='origin')\n",
    "        temp1['m'] = m[:,0]\n",
    "        dataStep2 = dataStep2.merge(temp1[['origin', 'm']], on='origin', how='left')\n",
    "        dataStep2.loc[dataStep2.m == 0, 'm'] = 0.0001\n",
    "\n",
    "        allV = {}\n",
    "        allV_arr = []\n",
    "\n",
    "        # estimate U(d) for each destination\n",
    "        for d in data['destination'].unique():\n",
    "\n",
    "            subData = dataStep2[dataStep2['destination'] == d]\n",
    "\n",
    "            X = subData['m']*subData[target+'jobs']*subData['F']\n",
    "            \n",
    "            y = subData[target]\n",
    "\n",
    "            model = sm.OLS(y,X).fit()\n",
    "\n",
    "            allV[d] = model.params[0]\n",
    "            allV_arr.append([model.params[0]]*len(subData))\n",
    "\n",
    "        binV = pd.DataFrame.from_dict(allV, orient='index', columns={'U'}).reset_index()\n",
    "        binV.rename(columns={'index':'destination'}, inplace=True)\n",
    "\n",
    "        # temporary dataframe with U\n",
    "        dataStep3 = dataStep2.merge(binV, on='destination', how='left') \n",
    "\n",
    "        # proportional fitting for U(d) for constraint sum(U) = sum(jobs)\n",
    "        Wtotal = [np.array(dataStep3.drop_duplicates(subset='destination')[target+'jobs'].rename('total'))]\n",
    "        dimensions1 = [[0]]\n",
    "        pad = len(max(allV_arr, key=len))\n",
    "        us = np.array([i + [0]*(pad-len(i)) for i in allV_arr])\n",
    "        IPF_1 = ipfn.ipfn(us, Wtotal, dimensions1)\n",
    "        n = IPF_1.iteration()\n",
    "        \n",
    "        ## final dataframe with F, U, V\n",
    "        temp2 = dataStep3.drop_duplicates(subset='destination')\n",
    "        temp2['n'] = n[:,0]\n",
    "        dataStep3 = dataStep3.merge(temp2[['destination', 'n']], on='destination', how='left')\n",
    "        dataStep3.loc[dataStep3.n == 0, 'n'] = 0.0001\n",
    "        \n",
    "        dataUV.append(dataStep3)\n",
    "    \n",
    "    if separate_income == False:\n",
    "        return dataF[0], dataUV[0]\n",
    "    else:\n",
    "        return dataF, dataUV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir constrainCTdistbinsNonlogIncome/\n",
    "cities = os.listdir('cities/')\n",
    "for city in cities:\n",
    "    if '.csv' in city:\n",
    "        df = pd.read_csv('cities/'+city)\n",
    "        dataF, dataUV = constrained_model_nonlog(getbins(df),separate_income=True)\n",
    "        dataUV = pd.concat([i for i in dataUV])\n",
    "        dataUV['flowPred'] = dataUV['F']*dataUV['m']*dataUV['n']\n",
    "        dataUV = dataUV.groupby(['origin','destination']).agg({'S000':sum,'flowPred':sum}).reset_index()\n",
    "        dataUV = dataUV.rename(columns={'SOOO','flowReal'})\n",
    "        dataUV.to_csv('constrainCTdistbinsNonlogIncome/'+city,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘constrainCTdistbinsNonlog/’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir constrainCTdistbinsNonlog/\n",
    "cities = os.listdir('cities/')\n",
    "for city in cities:\n",
    "    if '.csv' in city:\n",
    "        df = pd.read_csv('cities/'+city)\n",
    "        dataF, dataUV = constrained_model_nonlog(getbins(df),separate_income=False)\n",
    "        dataUV['flowPred'] = dataUV['F']*dataUV['m']*dataUV['n']\n",
    "        dataUV = dataUV.groupby(['origin','destination']).agg({'S000':sum,'flowPred':sum}).reset_index()\n",
    "        dataUV = dataUV.rename(columns={'SOOO','flowReal'})\n",
    "        dataUV.to_csv('constrainCTdistbinsNonlog/'+city,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE, doubly constrained, fit u,v together in iterations, from Mingyi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balancing(test,target,iterationNum,iteration = 20):\n",
    "#     print(target,'iteration', iterationNum)\n",
    "    if target+'B' not in test.columns:\n",
    "        test[target+'B'] = 1\n",
    "    test[target+'BDF'] = test[target+'jobs']*test[target+'f(d)']*test[target+'B']\n",
    "    if target+'A' in test.columns:\n",
    "        del test[target+'A']\n",
    "    del test[target+'B']\n",
    "    test = test.groupby(['origin']).agg({target+'BDF':sum}).\\\n",
    "    rename(columns={target+'BDF':target+'A'}).reset_index().\\\n",
    "    merge(test,on=['origin'],how='right')\n",
    "    test[target+'A'] = 1/test[target+'A']\n",
    "    test[target+'AOF'] = test[target+'residence']*test[target+'f(d)']*test[target+'A']\n",
    "    test = test.groupby(['destination']).agg({target+'AOF':sum}).\\\n",
    "    rename(columns={target+'AOF':target+'B'}).reset_index().\\\n",
    "    merge(test,on=['destination'],how='right')\n",
    "    test[target+'B'] = 1/test[target+'B']\n",
    "    test[target+'flowPred'] = test[target+'residence']*test[target+'jobs']*test[target+'f(d)']*\\\n",
    "                        test[target+'A']*test[target+'B']\n",
    "    \n",
    "    resultO = test[['origin',target+'residence']].drop_duplicates().\\\n",
    "    merge(test.groupby(['origin'])[[target+'flowPred']].sum().reset_index(),on=['origin'],how='left')\n",
    "    resultO['percentage'] = np.abs(resultO[target+'residence'] - resultO[target+'flowPred'])/resultO[target+'residence']\n",
    "    resultO = resultO['percentage'].mean()\n",
    "\n",
    "    resultD = test[['destination',target+'jobs']].drop_duplicates().\\\n",
    "    merge(test.groupby(['destination'])[[target+'flowPred']].sum().reset_index(),on=['destination'],how='left')\n",
    "    resultD['percentage'] = np.abs(resultD[target+'jobs'] - resultD[target+'flowPred'])/resultD[target+'jobs']\n",
    "    resultD = resultD['percentage'].mean()\n",
    "#     print(resultO,resultD)\n",
    "    if resultO < 0.05 and resultD < 0.05:\n",
    "        return test\n",
    "    else:\n",
    "        if iterationNum < iteration:\n",
    "            return balancing(test,target,iterationNum = iterationNum+1,iteration = 20)\n",
    "        else:\n",
    "            return test\n",
    "        \n",
    "def doubly_constrained_model_AB(data, separate_income=False):\n",
    "    \n",
    "    y_target = ['S000']  # target = total commute if no income segregation\n",
    "\n",
    "    \n",
    "    if separate_income == True:\n",
    "        \n",
    "        y_target = ['SE01', 'SE02', 'SE03'] # target = individual income commute if income segregation\n",
    "        \n",
    "        origin = data.groupby(['origin']).agg({'SE01':sum,'SE02':sum,'SE03':sum}).reset_index()\n",
    "        origin.columns = ['origin','SE01residence','SE02residence','SE03residence']\n",
    "        destination = data.groupby(['destination']).agg({'SE01':sum,'SE02':sum,'SE03':sum}).reset_index()\n",
    "        destination.columns = ['destination','SE01jobs','SE02jobs','SE03jobs']\n",
    "        data = data.merge(origin,on=['origin'])\n",
    "        data = data.merge(destination,on=['destination'])\n",
    "    \n",
    "    \n",
    "    targetOutput = []\n",
    "    for target in y_target:\n",
    "        binoutput = pd.DataFrame()\n",
    "        # estimate F for each bin\n",
    "        for b in data['bin'].unique():\n",
    "            \n",
    "            subData = data[data['bin'] == b]\n",
    "            X = subData[target+'residence'] * subData[target+'jobs']\n",
    "            \n",
    "            y = subData[target]\n",
    "\n",
    "            model = sm.OLS(y,X).fit()\n",
    "            \n",
    "            subData[target+'f(d)'] = model.params[0]       \n",
    "            binoutput = pd.concat([binoutput,subData])\n",
    "        binoutput = balancing(binoutput,target,iterationNum=1,iteration = 20)\n",
    "        \n",
    "        binoutput = binoutput[['origin','destination',target,target+'A',target+'B',target+'f(d)','bin',target+'flowPred']]\n",
    "        targetOutput.append(binoutput)\n",
    "    if separate_income == True:\n",
    "        targetOutput = targetOutput[0].merge(targetOutput[1],on=['origin','destination'],how='outer').\\\n",
    "                        merge(targetOutput[2],on=['origin','destination'],how='outer')\n",
    "    else:\n",
    "        targetOutput = targetOutput[0]\n",
    "    targetOutput = targetOutput.merge(data[['origin','destination','S000']])\n",
    "    return targetOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘constrainCTdistbinsABIncome/’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir constrainCTdistbinsABIncome/\n",
    "cities = os.listdir('cities/')\n",
    "for city in cities:\n",
    "    if '.csv' in city:\n",
    "        df = pd.read_csv('cities/'+city)\n",
    "        dataUV = doubly_constrained_model_AB(getbins(df),separate_income=True)\n",
    "        dataUV['flowPred'] = dataUV['SE01flowPred']+dataUV['SE02flowPred']+dataUV['SE03flowPred']\n",
    "        dataUV = dataUV.groupby(['origin','destination']).agg({'S000':sum,'flowPred':sum}).reset_index()\n",
    "        dataUV.to_csv('constrainCTdistbinsABIncome/'+city,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir constrainCTdistbinsAB/\n",
    "cities = os.listdir('cities/')\n",
    "for city in cities:\n",
    "    if '.csv' in city:\n",
    "        df = pd.read_csv('cities/'+city)\n",
    "        dataUV = doubly_constrained_model_AB(getbins(df),separate_income=False)\n",
    "        dataUV['flowPred'] = dataUV['SE01flowPred']+dataUV['SE02flowPred']+dataUV['SE03flowPred']\n",
    "        dataUV = dataUV.groupby(['origin','destination']).agg({'S000':sum,'flowPred':sum}).reset_index()\n",
    "        dataUV.to_csv('constrainCTdistbinsAB/'+city,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### unconstrain model, power law, from Mingyi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.optimize as optimize\n",
    "def power_law(x,k,a):\n",
    "    return k*((x[:,0]**a)*x[:,1]*x[:,2])\n",
    "def unconstrained_model(data, separate_income=False):\n",
    "    y_target = ['S000']  # target = total commute if no income segregation\n",
    "    data.loc[data.distance == 0, 'distance'] = 0.2\n",
    "    data.loc[data['SE01'] == 0, 'SE01'] = 0.1\n",
    "    data.loc[data['SE02'] == 0, 'SE02'] = 0.1\n",
    "    data.loc[data['SE03'] == 0, 'SE03'] = 0.1\n",
    "    origin = df.groupby(['origin']).agg({'S000':sum}).reset_index()\n",
    "    origin.columns = ['origin','S000residence']\n",
    "    destination = df.groupby(['destination']).agg({'S000':sum}).reset_index()\n",
    "    destination.columns = ['destination','S000jobs']\n",
    "    data = data.merge(origin,on=['origin'])\n",
    "    data = data.merge(destination,on=['destination'])\n",
    "    \n",
    "    if separate_income == True:\n",
    "        \n",
    "        y_target = ['SE01', 'SE02', 'SE03'] # target = individual income commute if income segregation\n",
    "        origin = df.groupby(['origin']).agg({'SE01':sum,'SE02':sum,'SE03':sum}).reset_index()\n",
    "        origin.columns = ['origin','SE01residence','SE02residence','SE03residence']\n",
    "        destination = df.groupby(['destination']).agg({'SE01':sum,'SE02':sum,'SE03':sum}).reset_index()\n",
    "        destination.columns = ['destination','SE01jobs','SE02jobs','SE03jobs']\n",
    "        data = data.merge(origin,on=['origin'])\n",
    "        data = data.merge(destination,on=['destination'])\n",
    "    dataF = []\n",
    "    for target in y_target:\n",
    "        X = data[['distance',target+'jobs',target+'residence']].values\n",
    "        y = data[target].values\n",
    "        pars, cov = optimize.curve_fit(f=power_law, xdata=X, ydata=y, bounds=(-np.inf, np.inf))\n",
    "#         print(pars)\n",
    "        data[target+'k'] = pars[0]\n",
    "        data[target+'a'] = pars[1]\n",
    "        data[target+'pred'] = data[target+'k']*(data['distance']**data[target+'a'])*data[target+'jobs']*data[target+'residence']\n",
    "    return data\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘unconstrainCTPowerlaw/’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir unconstrainCTPowerlaw/\n",
    "cities = os.listdir('cities/')\n",
    "for city in cities:\n",
    "    if '.csv' in city:\n",
    "        df = pd.read_csv('cities/'+city)\n",
    "        df = unconstrained_model(df,separate_income=False)\n",
    "        df.to_csv('unconstrainCTPowerlaw/'+city,index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir unconstrainCTPowerlawIncome/\n",
    "cities = os.listdir('cities/')\n",
    "for city in cities:\n",
    "    if '.csv' in city:\n",
    "        df = pd.read_csv('cities/'+city)\n",
    "        df = unconstrained_model(df,separate_income=True)\n",
    "        df.to_csv('unconstrainCTPowerlawIncome/'+city,index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### unconstrain model, full power law, from Mingyi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def power_law(x,k,a,b,c):\n",
    "    return k*(x[:,0]**a)*(x[:,1]**b)*(x[:,2]**c)\n",
    "def unconstrained_model(data, separate_income=False):\n",
    "    data.loc[data.distance == 0, 'distance'] = 0.2\n",
    "    data.loc[data['SE01'] == 0, 'SE01'] = 0.1\n",
    "    data.loc[data['SE02'] == 0, 'SE02'] = 0.1\n",
    "    data.loc[data['SE03'] == 0, 'SE03'] = 0.1\n",
    "    y_target = ['S000']  # target = total commute if no income segregation.2\n",
    "    origin = df.groupby(['origin']).agg({'S000':sum}).reset_index()\n",
    "    origin.columns = ['origin','S000residence']\n",
    "    destination = df.groupby(['destination']).agg({'S000':sum}).reset_index()\n",
    "    destination.columns = ['destination','S000jobs']\n",
    "    data = data.merge(origin,on=['origin'])\n",
    "    data = data.merge(destination,on=['destination'])\n",
    "    if separate_income == True:\n",
    "        \n",
    "        y_target = ['SE01', 'SE02', 'SE03'] # target = individual income commute if income segregation\n",
    "        origin = df.groupby(['origin']).agg({'SE01':sum,'SE02':sum,'SE03':sum}).reset_index()\n",
    "        origin.columns = ['origin','SE01residence','SE02residence','SE03residence']\n",
    "        destination = df.groupby(['destination']).agg({'SE01':sum,'SE02':sum,'SE03':sum}).reset_index()\n",
    "        destination.columns = ['destination','SE01jobs','SE02jobs','SE03jobs']\n",
    "        data = data.merge(origin,on=['origin'])\n",
    "        data = data.merge(destination,on=['destination'])\n",
    "    dataF = []\n",
    "    for target in y_target:\n",
    "        X = data[['distance',target+'jobs',target+'residence']].values\n",
    "        y = data[target].values\n",
    "        pars, cov = optimize.curve_fit(f=power_law, xdata=X, ydata=y, bounds=(-np.inf, np.inf))\n",
    "#         print(pars)\n",
    "        data[target+'k'] = pars[0]\n",
    "        data[target+'a'] = pars[1]\n",
    "        data[target+'b'] = pars[2]\n",
    "        data[target+'c'] = pars[3]\n",
    "        data[target+'pred'] = data[target+'k']*(data['distance']**data[target+'a'])*\\\n",
    "                        (data[target+'jobs']**data[target+'b'])*(data[target+'residence']**data[target+'c'])\n",
    "    return data\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir unconstrainCTFullPowerlaw/\n",
    "cities = os.listdir('cities/')\n",
    "for city in cities:\n",
    "    if '.csv' in city:\n",
    "        df = pd.read_csv('cities/'+city)\n",
    "        df = unconstrained_model(df,separate_income=False)\n",
    "        df.to_csv('unconstrainCTFullPowerlaw/'+city,index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir unconstrainCTFullPowerlawIncome/\n",
    "cities = os.listdir('cities/')\n",
    "for city in cities:\n",
    "    if '.csv' in city:\n",
    "        df = pd.read_csv('cities/'+city)\n",
    "        df = unconstrained_model(df,separate_income=True)\n",
    "        df.to_csv('unconstrainCTFullPowerlawIncome/'+city,index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### unconstrain model, exp, from Mingyi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.optimize as optimize\n",
    "def exp(x, a,b):\n",
    "    return a*(np.e**(b*x))\n",
    "def unconstrained_model(data, separate_income=False):\n",
    "    y_target = ['S000']  # target = total commute if no income segregation\n",
    "    data.loc[data.distance == 0, 'distance'] = 0.2\n",
    "    data.loc[data['SE01'] == 0, 'SE01'] = 0.1\n",
    "    data.loc[data['SE02'] == 0, 'SE02'] = 0.1\n",
    "    data.loc[data['SE03'] == 0, 'SE03'] = 0.1\n",
    "    origin = df.groupby(['origin']).agg({'S000':sum}).reset_index()\n",
    "    origin.columns = ['origin','S000residence']\n",
    "    destination = df.groupby(['destination']).agg({'S000':sum}).reset_index()\n",
    "    destination.columns = ['destination','S000jobs']\n",
    "    data = data.merge(origin,on=['origin'])\n",
    "    data = data.merge(destination,on=['destination'])\n",
    "    if separate_income == True:\n",
    "        \n",
    "        y_target = ['SE01', 'SE02', 'SE03'] \n",
    "        origin = df.groupby(['origin']).agg({'SE01':sum,'SE02':sum,'SE03':sum}).reset_index()\n",
    "        origin.columns = ['origin','SE01residence','SE02residence','SE03residence']\n",
    "        destination = df.groupby(['destination']).agg({'SE01':sum,'SE02':sum,'SE03':sum}).reset_index()\n",
    "        destination.columns = ['destination','SE01jobs','SE02jobs','SE03jobs']\n",
    "        data = data.merge(origin,on=['origin'])\n",
    "        data = data.merge(destination,on=['destination'])\n",
    "    dataF = []\n",
    "    for target in y_target:\n",
    "        X = data.distance.values\n",
    "        y = data[target]/(data[target+'jobs']*data[target+'residence'])\n",
    "        pars, cov = optimize.curve_fit(f=exp, xdata=X, ydata=y, bounds=(-np.inf, np.inf))\n",
    "#         print(pars)\n",
    "        data[target+'a'] = pars[0]\n",
    "        data[target+'b'] = pars[1]\n",
    "        data[target+'pred'] = data[target+'a']*(np.e**(data['distance']*data[target+'b']))*data[target+'jobs']*data[target+'residence']\n",
    "    return data\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir unconstrainCTExp/\n",
    "cities = os.listdir('cities/')\n",
    "for city in cities:\n",
    "    if '.csv' in city:\n",
    "        df = pd.read_csv('cities/'+city)\n",
    "        df = unconstrained_model(df,separate_income=False)\n",
    "        df.to_csv('unconstrainCTExp/'+city,index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir unconstrainCTExpIncome/\n",
    "cities = os.listdir('cities/')\n",
    "for city in cities:\n",
    "    if '.csv' in city:\n",
    "        df = pd.read_csv('cities/'+city)\n",
    "        df = unconstrained_model(df,separate_income=False)\n",
    "        df.to_csv('unconstrainCTExpIncome/'+city,index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
