{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from ipfn import ipfn\n",
    "from matplotlib import pyplot as plt\n",
    "from haversine import haversine_vector, Unit\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "from sttn.data.lehd import OriginDestinationEmploymentDataProvider\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "provider = OriginDestinationEmploymentDataProvider()\n",
    "\n",
    "import math\n",
    "from sttn.network import SpatioTemporalNetwork\n",
    "from sttn.utils import add_distance\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = [\n",
    "    ('New York City', 'ny', ['New York County, NY', 'Queens County, NY','Kings County, NY','Bronx County, NY','Richmond County, NY']),\n",
    "    ('Los Angeles', 'ca', ['Los Angeles County, CA']),\n",
    "    ('Chicago', 'il', ['Cook County, IL']),\n",
    "    ('Houston', 'tx', ['Harris County, TX']),\n",
    "    ('Boston', 'ma', ['Suffolk County, MA', 'Middlesex County, MA']),\n",
    "    ('Phoenix', 'az', ['Maricopa County, AZ']),\n",
    "    ('Philadelphia', 'pa', ['Philadelphia County, PA']),\n",
    "    ('San Antonio', 'tx', ['Bexar County, TX']),\n",
    "    ('San Diego', 'ca', ['San Diego County, CA']),\n",
    "    ('Dallas', 'tx', ['Dallas County, TX']),\n",
    "    ('San Jose', 'ca', ['Santa Clara County, CA']),\n",
    "    ('Austin', 'tx', ['Travis County, TX']),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_training_set(network, target_column):\n",
    "    # explode the dataset to include all node pairs\n",
    "    node_ids = network.nodes.index.values\n",
    "    origins = pd.DataFrame(node_ids, columns = ['origin'])\n",
    "    destinations = pd.DataFrame(node_ids, columns = ['destination'])\n",
    "    cartesian_product = origins.merge(destinations, how='cross')\n",
    "    \n",
    "    # compute distnace between all pairs\n",
    "    centroid = network.nodes.centroid\n",
    "    centroid_long = centroid.x\n",
    "    centroid_long.name = 'long'\n",
    "    centroid_lat = centroid.y\n",
    "    centroid_lat.name = 'lat'\n",
    "    centroids = pd.concat([centroid_long, centroid_lat], axis=1)\n",
    "    centroid_from = cartesian_product.join(centroids, on=network._origin).rename(columns={'long': 'long_from', 'lat': 'lat_from'})\n",
    "    centroid_all = centroid_from.join(centroids, on=network._destination).rename(columns={'long': 'long_to', 'lat': 'lat_to'})\n",
    "    from_points = list(zip(centroid_all.lat_from, centroid_all.long_from))\n",
    "    to_points = list(zip(centroid_all.lat_to, centroid_all.long_to))\n",
    "    centroid_all['distance'] = haversine_vector(from_points, to_points, Unit.KILOMETERS)\n",
    "    centroid_all.drop(['long_from', 'lat_from', 'long_to', 'lat_to'], axis=1, inplace=True)\n",
    "    centroid_all.loc[centroid_all.distance == 0, 'distance'] = 0.2\n",
    "    \n",
    "    # compute jobs and residence\n",
    "    comp_aggs={target_column: 'sum'}\n",
    "    jobs = city_network.agg_adjacent_edges(aggs=comp_aggs, outgoing=False).rename(columns={target_column: 'jobs'})\n",
    "    residence = city_network.agg_adjacent_edges(aggs=comp_aggs, outgoing=True).rename(columns={target_column: 'residence'})\n",
    "    features = centroid_all.join(residence, on='origin').join(jobs, on='destination')\n",
    "    \n",
    "    # merge flow data\n",
    "    flow = network.edges.rename(columns={target_column: 'flow'})[['origin', 'destination', 'flow']]\n",
    "    combined = features.merge(flow, how='left', on=['origin', 'destination']).fillna(0)\n",
    "    \n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘cities’: File exists\n",
      "New York City\n",
      "Los Angeles\n",
      "Chicago\n",
      "Houston\n",
      "Boston\n",
      "Phoenix\n",
      "Philadelphia\n",
      "San Antonio\n",
      "San Diego\n",
      "Dallas\n",
      "San Jose\n",
      "Austin\n"
     ]
    }
   ],
   "source": [
    "!mkdir cities\n",
    "target_columns = ['S000']\n",
    "for city, state, conties in cities:\n",
    "    print(city)\n",
    "    state_network = provider.get_data(state=state, year=2018)\n",
    "    city_network = state_network.filter_nodes(state_network.nodes.county.isin(conties))\n",
    "    for target_column in target_columns:\n",
    "        training_set = build_training_set(city_network, target_column)\n",
    "    training_set.to_csv('cities/%s.csv'%city,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>origin</th>\n",
       "      <th>destination</th>\n",
       "      <th>distance</th>\n",
       "      <th>residence</th>\n",
       "      <th>jobs</th>\n",
       "      <th>flow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>48453000402</td>\n",
       "      <td>48453000402</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1585</td>\n",
       "      <td>1087.0</td>\n",
       "      <td>41.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>48453000402</td>\n",
       "      <td>48453001912</td>\n",
       "      <td>11.122912</td>\n",
       "      <td>1585</td>\n",
       "      <td>4545.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>48453000402</td>\n",
       "      <td>48453001916</td>\n",
       "      <td>15.845681</td>\n",
       "      <td>1585</td>\n",
       "      <td>1083.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>48453000402</td>\n",
       "      <td>48453001783</td>\n",
       "      <td>15.750058</td>\n",
       "      <td>1585</td>\n",
       "      <td>459.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>48453000402</td>\n",
       "      <td>48453001918</td>\n",
       "      <td>9.294831</td>\n",
       "      <td>1585</td>\n",
       "      <td>4371.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47519</th>\n",
       "      <td>48453002310</td>\n",
       "      <td>48453002424</td>\n",
       "      <td>14.510079</td>\n",
       "      <td>627</td>\n",
       "      <td>558.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47520</th>\n",
       "      <td>48453002310</td>\n",
       "      <td>48453001742</td>\n",
       "      <td>29.977422</td>\n",
       "      <td>627</td>\n",
       "      <td>1417.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47521</th>\n",
       "      <td>48453002310</td>\n",
       "      <td>48453001760</td>\n",
       "      <td>26.177394</td>\n",
       "      <td>627</td>\n",
       "      <td>3080.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47522</th>\n",
       "      <td>48453002310</td>\n",
       "      <td>48453001826</td>\n",
       "      <td>18.175356</td>\n",
       "      <td>627</td>\n",
       "      <td>1233.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47523</th>\n",
       "      <td>48453002310</td>\n",
       "      <td>48453002310</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>627</td>\n",
       "      <td>603.0</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>47524 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            origin  destination   distance  residence    jobs  flow\n",
       "0      48453000402  48453000402   0.200000       1585  1087.0  41.0\n",
       "1      48453000402  48453001912  11.122912       1585  4545.0   5.0\n",
       "2      48453000402  48453001916  15.845681       1585  1083.0   3.0\n",
       "3      48453000402  48453001783  15.750058       1585   459.0   2.0\n",
       "4      48453000402  48453001918   9.294831       1585  4371.0   8.0\n",
       "...            ...          ...        ...        ...     ...   ...\n",
       "47519  48453002310  48453002424  14.510079        627   558.0   0.0\n",
       "47520  48453002310  48453001742  29.977422        627  1417.0   0.0\n",
       "47521  48453002310  48453001760  26.177394        627  3080.0   4.0\n",
       "47522  48453002310  48453001826  18.175356        627  1233.0   1.0\n",
       "47523  48453002310  48453002310   0.200000        627   603.0  22.0\n",
       "\n",
       "[47524 rows x 6 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### doubly constrained, fit u,v together in iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getbins(df, nbins=20):     \n",
    "    df['bin'] = pd.qcut(df['distance'], q=20)\n",
    "    df.sort_values(by='bin', inplace=True)\n",
    "    df.rename(columns={'jobs':'S000jobs', 'residence':'S000residence','flow':'S000flow'}, inplace=True)\n",
    "    \n",
    "    return df\n",
    "def balancing(test,target,iterationNum,iteration = 20):\n",
    "#     print(target,'iteration', iterationNum)\n",
    "    if target+'B' not in test.columns:\n",
    "        test[target+'B'] = 1\n",
    "    test[target+'BDF'] = test[target+'jobs']*test[target+'f(d)']*test[target+'B']\n",
    "    if target+'A' in test.columns:\n",
    "        del test[target+'A']\n",
    "    del test[target+'B']\n",
    "    test = test.groupby(['origin']).agg({target+'BDF':sum}).\\\n",
    "    rename(columns={target+'BDF':target+'A'}).reset_index().\\\n",
    "    merge(test,on=['origin'],how='right')\n",
    "    test[target+'A'] = 1/test[target+'A']\n",
    "    test[target+'AOF'] = test[target+'residence']*test[target+'f(d)']*test[target+'A']\n",
    "    test = test.groupby(['destination']).agg({target+'AOF':sum}).\\\n",
    "    rename(columns={target+'AOF':target+'B'}).reset_index().\\\n",
    "    merge(test,on=['destination'],how='right')\n",
    "    test[target+'B'] = 1/test[target+'B']\n",
    "    test[target+'pred'] = test[target+'residence']*test[target+'jobs']*test[target+'f(d)']*\\\n",
    "                        test[target+'A']*test[target+'B']\n",
    "    \n",
    "    resultO = test[['origin',target+'residence']].drop_duplicates().\\\n",
    "    merge(test.groupby(['origin'])[[target+'pred']].sum().reset_index(),on=['origin'],how='left')\n",
    "    resultO['percentage'] = np.abs(resultO[target+'residence'] - resultO[target+'pred'])/resultO[target+'residence']\n",
    "    resultO = resultO['percentage'].mean()\n",
    "\n",
    "    resultD = test[['destination',target+'jobs']].drop_duplicates().\\\n",
    "    merge(test.groupby(['destination'])[[target+'pred']].sum().reset_index(),on=['destination'],how='left')\n",
    "    resultD['percentage'] = np.abs(resultD[target+'jobs'] - resultD[target+'pred'])/resultD[target+'jobs']\n",
    "    resultD = resultD['percentage'].mean()\n",
    "#     print(resultO,resultD)\n",
    "    if resultO < 0.05 and resultD < 0.05:\n",
    "        return test\n",
    "    else:\n",
    "        if iterationNum < iteration:\n",
    "            return balancing(test,target,iterationNum = iterationNum+1,iteration = 20)\n",
    "        else:\n",
    "            return test\n",
    "        \n",
    "def doubly_constrained_model_AB(data,target):\n",
    "    \n",
    "\n",
    "    binoutput = pd.DataFrame()\n",
    "    # estimate F for each bin\n",
    "    for b in data['bin'].unique():\n",
    "        subData = data[data['bin'] == b]\n",
    "        X = subData[target+'residence'] * subData[target+'jobs']\n",
    "\n",
    "        y = subData[target+'flow']\n",
    "\n",
    "        model = sm.OLS(y,X).fit()\n",
    "\n",
    "        subData[target+'f(d)'] = model.params[0]       \n",
    "        binoutput = pd.concat([binoutput,subData])\n",
    "    binoutput = balancing(binoutput,target,iterationNum=1,iteration = 20)\n",
    "    binoutput = binoutput[['origin','destination',target+'flow',target+'pred']]\n",
    "\n",
    "#         binoutput = binoutput[['origin','destination',target+'flow',target+'A',target+'B',target+'f(d)','bin',target+'flowPred']]\n",
    "       \n",
    "\n",
    "    return binoutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘constrainCTdistbinsAB’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir constrainCTdistbinsAB\n",
    "outputDir = 'constrainCTdistbinsAB/'\n",
    "cities = os.listdir('cities/')\n",
    "separate_income = False\n",
    "for city in cities:\n",
    "    if '.csv' in city:\n",
    "        df = pd.read_csv('cities/'+city)\n",
    "        \n",
    "        dataUV = doubly_constrained_model_AB(getbins(df),'S000')\n",
    "        dataUV.to_csv(outputDir+city,index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### unconstrain model, power law"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.optimize as optimize\n",
    "def power_law(x,k,a):\n",
    "    return k*((x[:,0]**a)*x[:,1]*x[:,2])\n",
    "def unconstrained_powerlaw(data,target):\n",
    "\n",
    "    X = data[['distance',target+'jobs',target+'residence']].values\n",
    "    y = data[target+'flow'].values\n",
    "    pars, cov = optimize.curve_fit(f=power_law, xdata=X, ydata=y, bounds=(-np.inf, np.inf))\n",
    "#         print(pars)\n",
    "    data[target+'k'] = pars[0]\n",
    "    data[target+'a'] = pars[1]\n",
    "    data[target+'pred'] = data[target+'k']*(data['distance']**data[target+'a'])*data[target+'jobs']*data[target+'residence']\n",
    "    data = data[['origin', 'destination', target+'flow', target+'pred']]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘unconstrainCTPowerlaw/’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir unconstrainCTPowerlaw/\n",
    "cities = os.listdir('cities/')\n",
    "citiesDir = 'cities/'\n",
    "\n",
    "outputDir = 'unconstrainCTPowerlaw/'\n",
    "\n",
    "for city in cities:\n",
    "    if '.csv' in city:\n",
    "        df = pd.read_csv(citiesDir+city)\n",
    "        df.rename(columns={'jobs':'S000jobs', 'residence':'S000residence','flow':'S000flow'}, inplace=True)\n",
    "        dataUV = unconstrained_powerlaw(df,target='S000')\n",
    "        dataUV.to_csv(outputDir+city,index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### unconstrain model, full power law"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def power_law(x,k,a,b,c):\n",
    "    return k*(x[:,0]**a)*(x[:,1]**b)*(x[:,2]**c)\n",
    "def unconstrained_fullpowerlaw(data, target):\n",
    "    X = data[['distance',target+'jobs',target+'residence']].values\n",
    "    y = data[target+'flow'].values\n",
    "    pars, cov = optimize.curve_fit(f=power_law, xdata=X, ydata=y, bounds=(-np.inf, np.inf))\n",
    "#         print(pars)\n",
    "    data[target+'k'] = pars[0]\n",
    "    data[target+'a'] = pars[1]\n",
    "    data[target+'b'] = pars[2]\n",
    "    data[target+'c'] = pars[3]\n",
    "    data[target+'pred'] = data[target+'k']*(data['distance']**data[target+'a'])*\\\n",
    "                    (data[target+'jobs']**data[target+'b'])*(data[target+'residence']**data[target+'c'])\n",
    "    data = data[['origin', 'destination', target+'flow', target+'pred']]\n",
    "    return data\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘unconstrainCTFullPowerlaw/’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir unconstrainCTFullPowerlaw/\n",
    "cities = os.listdir('cities/')\n",
    "citiesDir = 'cities/'\n",
    "\n",
    "outputDir = 'unconstrainCTFullPowerlaw/'\n",
    "\n",
    "for city in cities:\n",
    "    if '.csv' in city:\n",
    "        df = pd.read_csv(citiesDir+city)\n",
    "        df.rename(columns={'jobs':'S000jobs', 'residence':'S000residence','flow':'S000flow'}, inplace=True)\n",
    "        dataUV = unconstrained_fullpowerlaw(df,target='S000')\n",
    "        dataUV.to_csv(outputDir+city,index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
