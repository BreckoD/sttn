{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from matplotlib import pyplot as plt\n",
    "from haversine import haversine_vector, Unit\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "from sttn.data.lehd import OriginDestinationEmploymentDataProvider\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "provider = OriginDestinationEmploymentDataProvider()\n",
    "\n",
    "import math\n",
    "from sttn.network import SpatioTemporalNetwork\n",
    "from sttn.utils import add_distance\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = [\n",
    "    ('New York City', 'ny', ['New York County, NY', 'Queens County, NY','Kings County, NY','Bronx County, NY','Richmond County, NY']),\n",
    "    ('Los Angeles', 'ca', ['Los Angeles County, CA']),\n",
    "    ('Chicago', 'il', ['Cook County, IL']),\n",
    "    ('Houston', 'tx', ['Harris County, TX']),\n",
    "    ('Boston', 'ma', ['Suffolk County, MA', 'Middlesex County, MA']),\n",
    "    ('Phoenix', 'az', ['Maricopa County, AZ']),\n",
    "    ('Philadelphia', 'pa', ['Philadelphia County, PA']),\n",
    "    ('San Antonio', 'tx', ['Bexar County, TX']),\n",
    "    ('San Diego', 'ca', ['San Diego County, CA']),\n",
    "    ('Dallas', 'tx', ['Dallas County, TX']),\n",
    "    ('San Jose', 'ca', ['Santa Clara County, CA']),\n",
    "    ('Austin', 'tx', ['Travis County, TX']),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_training_set(network, target_column):\n",
    "    # explode the dataset to include all node pairs\n",
    "    node_ids = network.nodes.index.values\n",
    "    origins = pd.DataFrame(node_ids, columns = ['origin'])\n",
    "    destinations = pd.DataFrame(node_ids, columns = ['destination'])\n",
    "    cartesian_product = origins.merge(destinations, how='cross')\n",
    "    \n",
    "    # compute distnace between all pairs\n",
    "    centroid = network.nodes.centroid\n",
    "    centroid_long = centroid.x\n",
    "    centroid_long.name = 'long'\n",
    "    centroid_lat = centroid.y\n",
    "    centroid_lat.name = 'lat'\n",
    "    centroids = pd.concat([centroid_long, centroid_lat], axis=1)\n",
    "    centroid_from = cartesian_product.join(centroids, on=network._origin).rename(columns={'long': 'long_from', 'lat': 'lat_from'})\n",
    "    centroid_all = centroid_from.join(centroids, on=network._destination).rename(columns={'long': 'long_to', 'lat': 'lat_to'})\n",
    "    from_points = list(zip(centroid_all.lat_from, centroid_all.long_from))\n",
    "    to_points = list(zip(centroid_all.lat_to, centroid_all.long_to))\n",
    "    centroid_all['distance'] = haversine_vector(from_points, to_points, Unit.KILOMETERS)\n",
    "    centroid_all.drop(['long_from', 'lat_from', 'long_to', 'lat_to'], axis=1, inplace=True)\n",
    "    centroid_all.loc[centroid_all.distance == 0, 'distance'] = 0.2\n",
    "    \n",
    "    # compute jobs and residence\n",
    "    comp_aggs={target_column: 'sum'}\n",
    "    jobs = city_network.agg_adjacent_edges(aggs=comp_aggs, outgoing=False).rename(columns={target_column: 'jobs'})\n",
    "    residence = city_network.agg_adjacent_edges(aggs=comp_aggs, outgoing=True).rename(columns={target_column: 'residence'})\n",
    "    features = centroid_all.join(residence, on='origin').join(jobs, on='destination')\n",
    "    \n",
    "    # merge flow data\n",
    "    flow = network.edges.rename(columns={target_column: 'flow'})[['origin', 'destination', 'flow']]\n",
    "    combined = features.merge(flow, how='left', on=['origin', 'destination']).fillna(0)\n",
    "    \n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New York City\n",
      "Los Angeles\n",
      "Chicago\n",
      "Houston\n",
      "Boston\n",
      "Phoenix\n",
      "Philadelphia\n",
      "San Antonio\n",
      "San Diego\n",
      "Dallas\n",
      "San Jose\n",
      "Austin\n"
     ]
    }
   ],
   "source": [
    "!mkdir cities\n",
    "target_columns = ['S000']\n",
    "for city, state, conties in cities:\n",
    "    print(city)\n",
    "    state_network = provider.get_data(state=state, year=2018)\n",
    "    city_network = state_network.filter_nodes(state_network.nodes.county.isin(conties))\n",
    "    for target_column in target_columns:\n",
    "        training_set = build_training_set(city_network, target_column)\n",
    "    training_set.to_csv('cities/%s.csv'%city,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New York City 4.541835596920669\n",
      "Los Angeles 5.402262622824809\n",
      "Chicago 15.425267699640795\n",
      "Houston 31.99232714601695\n",
      "Boston 60.808485063728334\n",
      "Phoenix 16.651786719782333\n",
      "Philadelphia 18.109686675927406\n",
      "San Antonio 41.2929869638189\n",
      "San Diego 50.41335390462399\n",
      "Dallas 25.59896030075406\n",
      "San Jose 46.2919221741127\n",
      "Austin 78.46324185307752\n"
     ]
    }
   ],
   "source": [
    "for city, state, conties in cities:\n",
    "    df = pd.read_csv(f'cities/{city}.csv')\n",
    "    \n",
    "    bins = 17\n",
    "    df['bin'] = pd.qcut(df['distance'], q=bins)\n",
    "    test_set = df.sample(frac=0.2, random_state=1)\n",
    "    training_set = df.drop(test_set.index)\n",
    "    \n",
    "    model = fit_bucketed_power_law(training_set) #fit_unconstrained_powerlaw(training_set)\n",
    "    prediction_results = predict_bucketed_power_law(test_set, model) #predict_unconstrained_powerlaw(test_set, model)\n",
    "    mse = mean_squared_error(prediction_results.flow, prediction_results.prediction)\n",
    "    print(city, mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### doubly constrained, fit u,v together in iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getbins(df, nbins=20):     \n",
    "    df['bin'] = pd.qcut(df['distance'], q=20)\n",
    "    df.sort_values(by='bin', inplace=True)\n",
    "    df.rename(columns={'jobs':'S000jobs', 'residence':'S000residence','flow':'S000flow'}, inplace=True)\n",
    "    \n",
    "    return df\n",
    "def balancing(test,target,iterationNum,iteration = 20):\n",
    "#     print(target,'iteration', iterationNum)\n",
    "    if target+'B' not in test.columns:\n",
    "        test[target+'B'] = 1\n",
    "    test[target+'BDF'] = test[target+'jobs']*test[target+'f(d)']*test[target+'B']\n",
    "    if target+'A' in test.columns:\n",
    "        del test[target+'A']\n",
    "    del test[target+'B']\n",
    "    test = test.groupby(['origin']).agg({target+'BDF':sum}).\\\n",
    "    rename(columns={target+'BDF':target+'A'}).reset_index().\\\n",
    "    merge(test,on=['origin'],how='right')\n",
    "    test[target+'A'] = 1/test[target+'A']\n",
    "    test[target+'AOF'] = test[target+'residence']*test[target+'f(d)']*test[target+'A']\n",
    "    test = test.groupby(['destination']).agg({target+'AOF':sum}).\\\n",
    "    rename(columns={target+'AOF':target+'B'}).reset_index().\\\n",
    "    merge(test,on=['destination'],how='right')\n",
    "    test[target+'B'] = 1/test[target+'B']\n",
    "    test[target+'pred'] = test[target+'residence']*test[target+'jobs']*test[target+'f(d)']*\\\n",
    "                        test[target+'A']*test[target+'B']\n",
    "    \n",
    "    resultO = test[['origin',target+'residence']].drop_duplicates().\\\n",
    "    merge(test.groupby(['origin'])[[target+'pred']].sum().reset_index(),on=['origin'],how='left')\n",
    "    resultO['percentage'] = np.abs(resultO[target+'residence'] - resultO[target+'pred'])/resultO[target+'residence']\n",
    "    resultO = resultO['percentage'].mean()\n",
    "\n",
    "    resultD = test[['destination',target+'jobs']].drop_duplicates().\\\n",
    "    merge(test.groupby(['destination'])[[target+'pred']].sum().reset_index(),on=['destination'],how='left')\n",
    "    resultD['percentage'] = np.abs(resultD[target+'jobs'] - resultD[target+'pred'])/resultD[target+'jobs']\n",
    "    resultD = resultD['percentage'].mean()\n",
    "#     print(resultO,resultD)\n",
    "    if resultO < 0.02 and resultD < 0.02:\n",
    "        return test\n",
    "    else:\n",
    "        if iterationNum < iteration:\n",
    "            return balancing(test,target,iterationNum = iterationNum+1,iteration = 20)\n",
    "        else:\n",
    "            return test\n",
    "        \n",
    "def doubly_constrained_model_AB(data,target):\n",
    "    \n",
    "\n",
    "    binoutput = pd.DataFrame()\n",
    "    # estimate F for each bin\n",
    "    for b in data['bin'].unique():\n",
    "        subData = data[data['bin'] == b]\n",
    "        X = subData[target+'residence'] * subData[target+'jobs']\n",
    "\n",
    "        y = subData[target+'flow']\n",
    "\n",
    "        model = sm.OLS(y,X).fit()\n",
    "\n",
    "        subData[target+'f(d)'] = model.params[0]       \n",
    "        binoutput = pd.concat([binoutput,subData])\n",
    "    binoutput = balancing(binoutput,target,iterationNum=1,iteration = 20)\n",
    "    binoutput = binoutput[['origin','destination',target+'flow',target+'pred',target+'f(d)','bin']]\n",
    "\n",
    "#         binoutput = binoutput[['origin','destination',target+'flow',target+'A',target+'B',target+'f(d)','bin',target+'flowPred']]\n",
    "       \n",
    "\n",
    "    return binoutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir constrainCTdistbinsAB\n",
    "fdf = []\n",
    "outputDir = 'constrainCTdistbinsAB/'\n",
    "cities = os.listdir('cities/')\n",
    "separate_income = False\n",
    "target = 'S000'\n",
    "for city in cities:\n",
    "    if '.csv' in city:\n",
    "        df = pd.read_csv('cities/'+city)\n",
    "        dataUV = doubly_constrained_model_AB(getbins(df),target)\n",
    "        dataUV[['origin','destination',target+'flow',target+'pred']].to_csv(outputDir+city,index=False)\n",
    "        grouped_bfpl = dataUV.groupby('bin').agg({target+'f(d)': 'min'})\n",
    "        grouped_bfpl = grouped_bfpl.reset_index()\n",
    "        fdf += [grouped_bfpl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for grouped_bfpl in fdf:\n",
    "    plt.plot(grouped_bfpl[target+'f(d)'])\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### unconstrain model, power law"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.optimize as optimize\n",
    "def power_law(x,k,a):\n",
    "    return k*((x[:,0]**a)*x[:,1]*x[:,2])\n",
    "\n",
    "def fit_unconstrained_powerlaw(data):\n",
    "    X = data[['distance', 'jobs', 'residence']].values\n",
    "    y = data['flow'].values\n",
    "    pars, cov = optimize.curve_fit(f=power_law, xdata=X, ydata=y, bounds=(-np.inf, np.inf))\n",
    "    return pars\n",
    "\n",
    "def predict_unconstrained_powerlaw(data, model):\n",
    "    k = model[0]\n",
    "    a =  model[1]\n",
    "    data['prediction'] = (data['distance']**a)*data['jobs']*data['residence']*k\n",
    "    return data\n",
    "\n",
    "def unconstrained_powerlaw(data,target):\n",
    "\n",
    "    X = data[['distance',target+'jobs',target+'residence']].values\n",
    "    y = data[target+'flow'].values\n",
    "    pars, cov = optimize.curve_fit(f=power_law, xdata=X, ydata=y, bounds=(-np.inf, np.inf))\n",
    "#         print(pars)\n",
    "    data[target+'k'] = pars[0]\n",
    "    data[target+'a'] = pars[1]\n",
    "    data[target+'pred'] = 1\n",
    "    #data = data[['origin', 'destination', target+'flow', target+'pred']]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set.rename(columns={'jobs':'S000jobs', 'residence':'S000residence','flow':'S000flow'}, inplace=True)\n",
    "predicted_pl = unconstrained_powerlaw(training_set, 'S000')\n",
    "mean_squared_error(predicted_pl.S000flow, predicted_pl.S000pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir unconstrainCTPowerlaw/\n",
    "cities = os.listdir('cities/')\n",
    "citiesDir = 'cities/'\n",
    "\n",
    "outputDir = 'unconstrainCTPowerlaw/'\n",
    "\n",
    "for city in cities:\n",
    "    if '.csv' in city:\n",
    "        df = pd.read_csv(citiesDir+city)\n",
    "        df.rename(columns={'jobs':'S000jobs', 'residence':'S000residence','flow':'S000flow'}, inplace=True)\n",
    "        dataUV = unconstrained_powerlaw(df,target='S000')\n",
    "        dataUV.to_csv(outputDir+city,index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### unconstrain model, full power law"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def power_law(x,k,a,b,c):\n",
    "    return k*(x[:,0]**a)*(x[:,1]**b)*(x[:,2]**c)\n",
    "def unconstrained_fullpowerlaw(data, target):\n",
    "    X = data[['distance',target+'jobs',target+'residence']].values\n",
    "    y = data[target+'flow'].values\n",
    "    pars, cov = optimize.curve_fit(f=power_law, xdata=X, ydata=y, bounds=(-np.inf, np.inf))\n",
    "#         print(pars)\n",
    "    data[target+'k'] = pars[0]\n",
    "    data[target+'a'] = pars[1]\n",
    "    data[target+'b'] = pars[2]\n",
    "    data[target+'c'] = pars[3]\n",
    "    data[target+'pred'] = data[target+'k']*(data['distance']**data[target+'a'])*\\\n",
    "                    (data[target+'jobs']**data[target+'b'])*(data[target+'residence']**data[target+'c'])\n",
    "    #data = data[['origin', 'destination', target+'flow', target+'pred']]\n",
    "    return data\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set.rename(columns={'jobs':'S000jobs', 'residence':'S000residence','flow':'S000flow'}, inplace=True)\n",
    "predicted_pl = unconstrained_fullpowerlaw(training_set, 'S000')\n",
    "mean_squared_error(predicted_pl.S000flow, predicted_pl.S000pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir unconstrainCTFullPowerlaw/\n",
    "cities = os.listdir('cities/')\n",
    "citiesDir = 'cities/'\n",
    "\n",
    "outputDir = 'unconstrainCTFullPowerlaw/'\n",
    "\n",
    "for city in cities:\n",
    "    if '.csv' in city:\n",
    "        df = pd.read_csv(citiesDir+city)\n",
    "        df.rename(columns={'jobs':'S000jobs', 'residence':'S000residence','flow':'S000flow'}, inplace=True)\n",
    "        dataUV = unconstrained_fullpowerlaw(df,target='S000')\n",
    "        dataUV.to_csv(outputDir+city,index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bucketed unconstrained power law"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_bucketed_power_law(data, bins = 20):\n",
    "    bins_list = data['bin'].unique()    \n",
    "    bin_to_model = {}\n",
    "    for b in bins_list:\n",
    "        subData = data[data['bin'] == b]\n",
    "        model = fit_unconstrained_powerlaw(subData)\n",
    "        bin_to_model[b] = model\n",
    "        \n",
    "    return bin_to_model\n",
    "    \n",
    "def predict_bucketed_power_law(data, model):\n",
    "    bins_list = data['bin'].unique()\n",
    "    predictions_all = pd.DataFrame()\n",
    "    for b in bins_list:\n",
    "        bin_model = model[b]\n",
    "        subData = data[data['bin'] == b]\n",
    "        predicted = predict_unconstrained_powerlaw(subData, bin_model)\n",
    "        predictions_all = pd.concat([predictions_all, predicted])\n",
    "        \n",
    "    return predictions_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set.rename(columns={'jobs':'S000jobs', 'residence':'S000residence','flow':'S000flow'}, inplace=True)\n",
    "predicted_bfpl = bucketed_power_law(training_set, bins=20)\n",
    "grouped_bfpl = predicted_bfpl.groupby('bin').agg({'S000k': 'min', 'S000a': 'min'})\n",
    "grouped_bfpl = grouped_bfpl.reset_index()\n",
    "mean_squared_error(predicted_bfpl.S000flow, predicted_bfpl.S000pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(d, boundaries):\n",
    "    for index, row in boundaries.iterrows():\n",
    "        if d in row['bin']:\n",
    "            return row['S000k']*(d**row['S000a'])\n",
    "\n",
    "x=list(range(1, 50))\n",
    "y = []\n",
    "for i in x:\n",
    "  y.append(f(i, grouped_bfpl)) \n",
    "\n",
    "plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bucketed unconstrained power law without distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def power_law(x,k,a,b,c):\n",
    "    return k*(x[:,0]**a)*(x[:,1]**b)*(x[:,2]**c)\n",
    "def bucked_power_law(x,f):\n",
    "    return f*(x[:,0]**x[:,1])*(x[:,2]**x[:,3])*x[:,4]\n",
    "def unconstrained_bucked_fullpowerlaw(data, target,bins):\n",
    "    X = data[['distance',target+'jobs',target+'residence']].values\n",
    "    y = data[target+'flow'].values\n",
    "    pars, cov = optimize.curve_fit(f=power_law, xdata=X, ydata=y, bounds=(-np.inf, np.inf))\n",
    "#         print(pars)\n",
    "    data[target+'k'] = pars[0]\n",
    "    data[target+'a'] = pars[1]\n",
    "    data[target+'b'] = pars[2]\n",
    "    data[target+'c'] = pars[3]\n",
    "    data[target+'pred'] = data[target+'k']*(data['distance']**data[target+'a'])*\\\n",
    "                    (data[target+'jobs']**data[target+'b'])*(data[target+'residence']**data[target+'c'])\n",
    "    data['bin'] = pd.qcut(data['distance'], q=bins)\n",
    "    predictions_all = pd.DataFrame()\n",
    "    # estimate F for each bin\n",
    "    for b in data['bin'].unique():\n",
    "        subData = data[data['bin'] == b]\n",
    "        X = subData[[target+'jobs',target+'b',target+'residence',target+'c',target+'k']].values\n",
    "        y = subData[target+'flow'].values\n",
    "        pars, cov = optimize.curve_fit(f=bucked_power_law, xdata=X, ydata=y, bounds=(-np.inf, np.inf))\n",
    "        subData[target+'f'] = pars[0]\n",
    "        subData[target+'pred'] = subData[target+'f']*subData[target+'k']*\\\n",
    "                (subData[target+'jobs']**subData[target+'b'])*(subData[target+'residence']**subData[target+'c'])\n",
    "        predictions_all = pd.concat([predictions_all, subData])\n",
    "        \n",
    "    return predictions_all\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!mkdir unconstrainCTBuckedFullPowerlaw/\n",
    "cities = os.listdir('cities/')\n",
    "citiesDir = 'cities/'\n",
    "\n",
    "outputDir = 'unconstrainCTBuckedFullPowerlaw/'\n",
    "fdf = []\n",
    "for city in cities:\n",
    "    if '.csv' in city:\n",
    "        print(city)\n",
    "        df = pd.read_csv(citiesDir+city)\n",
    "        df.rename(columns={'jobs':'S000jobs', 'residence':'S000residence','flow':'S000flow'}, inplace=True)\n",
    "        predicted_bfpl = unconstrained_bucked_fullpowerlaw(df, 'S000',bins=20)\n",
    "        predicted_bfpl[['origin', 'destination','S000flow','S000pred']].to_csv(outputDir+city,index=False)\n",
    "        \n",
    "        grouped_bfpl = predicted_bfpl.groupby('bin').agg({'S000f': 'min'})\n",
    "        grouped_bfpl = grouped_bfpl.reset_index()\n",
    "        fdf += [grouped_bfpl]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for grouped_bfpl in fdf:\n",
    "    plt.plot(grouped_bfpl['S000f'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def power_law(x,k,a):\n",
    "    return k*((x[:,0]**a)*x[:,1]*x[:,2])\n",
    "def bucked_power_law(x,f):\n",
    "    return f*(x[:,0]*x[:,1]*x[:,2])\n",
    "def unconstrained_bucked_powerlaw(data, target,bins):\n",
    "    X = data[['distance',target+'jobs',target+'residence']].values\n",
    "    y = data[target+'flow'].values\n",
    "    pars, cov = optimize.curve_fit(f=power_law, xdata=X, ydata=y, bounds=(-np.inf, np.inf))\n",
    "#         print(pars)\n",
    "    data[target+'k'] = pars[0]\n",
    "    data[target+'a'] = pars[1]\n",
    "    data['bin'] = pd.qcut(data['distance'], q=bins)\n",
    "    predictions_all = pd.DataFrame()\n",
    "    # estimate F for each bin\n",
    "    for b in data['bin'].unique():\n",
    "        subData = data[data['bin'] == b]\n",
    "        X = subData[[target+'jobs',target+'residence',target+'k']].values\n",
    "        y = subData[target+'flow'].values\n",
    "        pars, cov = optimize.curve_fit(f=bucked_power_law, xdata=X, ydata=y, bounds=(-np.inf, np.inf))\n",
    "        subData[target+'f'] = pars[0]\n",
    "        subData[target+'pred'] = subData[target+'f']*subData[target+'k']*\\\n",
    "                subData[target+'jobs']*subData[target+'residence']\n",
    "        predictions_all = pd.concat([predictions_all, subData])\n",
    "        \n",
    "    return predictions_all\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_bfpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!mkdir unconstrainCTBuckedPowerlaw/\n",
    "cities = os.listdir('cities/')\n",
    "citiesDir = 'cities/'\n",
    "fdf = []\n",
    "outputDir = 'unconstrainCTBuckedPowerlaw/'\n",
    "\n",
    "for city in cities:\n",
    "    if '.csv' in city:\n",
    "        print(city)\n",
    "        df = pd.read_csv(citiesDir+city)\n",
    "        df.rename(columns={'jobs':'S000jobs', 'residence':'S000residence','flow':'S000flow'}, inplace=True)\n",
    "        predicted_bfpl = unconstrained_bucked_powerlaw(df,'S000', bins=20)\n",
    "        predicted_bfpl[['origin', 'destination','S000flow','S000pred']].to_csv(outputDir+city,index=False)\n",
    "        grouped_bfpl = predicted_bfpl.groupby('bin').agg({'S000f': 'min'})\n",
    "        grouped_bfpl = grouped_bfpl.reset_index()\n",
    "        fdf += [grouped_bfpl]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for grouped_bfpl in fdf:\n",
    "    plt.plot(grouped_bfpl['S000f'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
